{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB735pYV7wSv",
        "outputId": "8c9fe5e6-0ddf-49bb-a001-d2014e20624e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q font-roboto\n",
        "!pip install -q git+https://github.com/googlefonts/fontmake.git\n",
        "!wget -q https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf\n",
        "!wget -q https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Bold.otf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR7lGT1C8FQP",
        "outputId": "34ab3ad7-dca2-4def-da8d-944d0b39688b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m2.1/2.4 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for font-roboto (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m903.4/903.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.8/159.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.7/613.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m844.3/844.3 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.3/526.3 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fontmake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fm.fontManager.addfont('NotoSansCJKsc-Regular.otf')\n",
        "fm.fontManager.addfont('NotoSansCJKsc-Bold.otf')\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n"
      ],
      "metadata": {
        "id": "Y3a5O8An71h0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "rJVKnuSx7W-A",
        "outputId": "b8d9fbcc-da4b-42f8-f3e8-2937c8bbe80e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/9417pro/X_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4216b76a71af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/9417pro/X_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/9417pro/y_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/9417pro/X_train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X_train = pd.read_csv('/content/drive/MyDrive/9417pro/X_train.csv').values\n",
        "y_train = pd.read_csv('/content/drive/MyDrive/9417pro/y_train.csv')['label'].values\n",
        "\n",
        "\n",
        "# 1. 数据分析\n",
        "# ----------------------------------\n",
        "print(\"训练集形状:\", X_train.shape)\n",
        "print(\"标签形状:\", y_train.shape)\n",
        "\n",
        "label_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "print(\"类别分布:\")\n",
        "print(label_counts)\n",
        "imbalance_ratio = label_counts.max() / label_counts.min()\n",
        "print(\"最大类别与最小类别的不平衡比例:\", imbalance_ratio)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')\n",
        "plt.title(\"训练集类别分布\")\n",
        "plt.xlabel(\"类别\")\n",
        "plt.ylabel(\"样本数\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 2. HCBOU\n",
        "# -------------------------------\n",
        "\n",
        "# SMOTE-like合成样本生成函数 局部内插\n",
        "def generate_synthetic_samples(X_cluster, n_samples, k_neighbors=5, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    n_samples_cluster = X_cluster.shape[0]\n",
        "    if n_samples_cluster < 2:\n",
        "        # 如果簇内样本数太少，则直接复制已有样本\n",
        "        return np.tile(X_cluster, (n_samples, 1))\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=min(k_neighbors, n_samples_cluster)).fit(X_cluster)\n",
        "    synthetic_samples = []\n",
        "    for _ in range(n_samples):\n",
        "        idx = np.random.randint(0, n_samples_cluster)\n",
        "        sample = X_cluster[idx]\n",
        "        # 获取最近\n",
        "        distances, indices = nn.kneighbors([sample])\n",
        "        if len(indices[0]) > 1:\n",
        "            # 随机选择一个邻居\n",
        "            neighbor_idx = np.random.choice(indices[0][1:])\n",
        "        else:\n",
        "            neighbor_idx = indices[0][0]\n",
        "        neighbor = X_cluster[neighbor_idx]\n",
        "        lam = np.random.rand()\n",
        "        synthetic = sample + lam * (neighbor - sample)\n",
        "        synthetic_samples.append(synthetic)\n",
        "    return np.array(synthetic_samples)\n",
        "\n",
        "def hc_bou_resample(X, y, random_state=42, minority_k=3):\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    unique_labels = np.unique(y)\n",
        "    C = len(unique_labels)\n",
        "    N = X.shape[0]\n",
        "    S = int(np.floor(N / C))\n",
        "    print(f\"目标每类样本数 S: {S}\")\n",
        "\n",
        "    X_resampled_list = []\n",
        "    y_resampled_list = []\n",
        "\n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y == label)[0]\n",
        "        X_class = X[idx]\n",
        "        n_class = X_class.shape[0]\n",
        "        # 多数类处理：欠采样\n",
        "        if n_class >= S:\n",
        "            print(f\"类别 {label} 为多数类, 原始样本数: {n_class}\")\n",
        "            # 采用KMeans聚类，选取 S 个聚类中心作为代表\n",
        "            km = KMeans(n_clusters=S, random_state=random_state)\n",
        "            km.fit(X_class)\n",
        "            X_new = km.cluster_centers_\n",
        "            y_new = np.full(S, label)\n",
        "        else:\n",
        "            print(f\"类别 {label} 为少数类, 原始样本数: {n_class}\")\n",
        "            # 先进行少数类内部聚类，设定聚类数 minority_k\n",
        "            km = KMeans(n_clusters=min(minority_k, n_class), random_state=random_state)\n",
        "            km.fit(X_class)\n",
        "            labels_cluster = km.labels_\n",
        "            X_new = X_class.copy()  # 原有样本保留\n",
        "            # 计算合成样本总数需要补充\n",
        "            n_to_generate = S - n_class\n",
        "            synth_samples_all = []\n",
        "            # 对于各簇，按样本占比分配生成数量\n",
        "            for cluster in np.unique(labels_cluster):\n",
        "                cluster_idx = np.where(labels_cluster == cluster)[0]\n",
        "                X_cluster = X_class[cluster_idx]\n",
        "                weight = X_cluster.shape[0] / n_class\n",
        "                n_synth = int(np.floor(weight * n_to_generate))\n",
        "                if n_synth > 0:\n",
        "                    synth_samples = generate_synthetic_samples(X_cluster, n_synth, k_neighbors=5, random_state=random_state)\n",
        "                    synth_samples_all.append(synth_samples)\n",
        "            if synth_samples_all:\n",
        "                synth_samples_all = np.vstack(synth_samples_all)\n",
        "                X_new = np.vstack([X_new, synth_samples_all])\n",
        "            else:\n",
        "                # 若未生成任何新样本，则简单复制随机样本\n",
        "                duplicates = X_class[np.random.choice(n_class, n_to_generate, replace=True)]\n",
        "                X_new = np.vstack([X_new, duplicates])\n",
        "            y_new = np.full(X_new.shape[0], label)\n",
        "\n",
        "        X_resampled_list.append(X_new)\n",
        "        y_resampled_list.append(y_new)\n",
        "\n",
        "    # 合并所有类别\n",
        "    X_resampled = np.vstack(X_resampled_list)\n",
        "    y_resampled = np.hstack(y_resampled_list)\n",
        "    print(\"重采样后总样本数:\", X_resampled.shape[0])\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "\n",
        "# 3. 应用\n",
        "# ----------------------------------\n",
        "\n",
        "# HCBOU算法重采样\n",
        "X_resampled, y_resampled = hc_bou_resample(X_train, y_train, random_state=42, minority_k=3)\n",
        "\n",
        "# 检查新数据分布\n",
        "resampled_label_counts = pd.Series(y_resampled).value_counts().sort_index()\n",
        "print(\"重采样后各类别分布:\")\n",
        "print(resampled_label_counts)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=resampled_label_counts.index, y=resampled_label_counts.values, palette='viridis')\n",
        "plt.title(\"重采样后训练集类别分布\")\n",
        "plt.xlabel(\"类别\")\n",
        "plt.ylabel(\"样本数\")\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. 建模（随机森林验证）\n",
        "# ----------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "scores = cross_val_score(clf, X_resampled, y_resampled, cv=5, scoring='f1_macro')\n",
        "print(\"使用重采样数据后随机森林的 F1-macro 交叉验证得分:\", scores.mean())\n"
      ]
    }
  ]
}